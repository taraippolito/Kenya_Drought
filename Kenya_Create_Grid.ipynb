{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b5f7462",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import rasterio \n",
    "import xarray as xr\n",
    "import earthpy as et\n",
    "from multiprocessing import Pool\n",
    "from parallel_xarray import sample_tif\n",
    "from get_dynamic_by_date import get_date_df, get_SAVI_date_df\n",
    "from istarmap import istarmap\n",
    "import tqdm\n",
    "import pandas as pd \n",
    "# import elevation\n",
    "# import richdem as rd\n",
    "from shapely import geometry\n",
    "import datetime\n",
    "from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d7fc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colorado\\.conda\\envs\\earth-analytics-python\\lib\\site-packages\\geopandas\\io\\file.py:299: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "# Create a grid of points - 250m spacing \n",
    "\n",
    "# Read the shapefile\n",
    "df = gpd.read_file('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok.shp')\n",
    "# Reproject to projected coordinate system\n",
    "df = df.to_crs('EPSG:3857')\n",
    "# 250m spacing\n",
    "spacing = 250\n",
    "# get the bounds\n",
    "xmin, ymin, xmax, ymax = df.total_bounds\n",
    "\n",
    "# pull the x and y coordinates\n",
    "xcoords = [i for i in np.arange(xmin, xmax, spacing)]\n",
    "ycoords = [i for i in np.arange(ymin, ymax, spacing)]\n",
    "\n",
    "pointcoords = np.array(np.meshgrid(xcoords, ycoords)).T.reshape(-1, 2) #A 2D array like [[x1,y1], [x1,y2], ...\n",
    "points = gpd.points_from_xy(x=pointcoords[:,0], y=pointcoords[:,1])\n",
    "grid = gpd.GeoSeries(points, crs=df.crs)\n",
    "grid.name = 'geometry'\n",
    "\n",
    "#If you just want to points inside polygons:\n",
    "gridinside = gpd.sjoin(gpd.GeoDataFrame(grid), df[['geometry']], how=\"inner\")\n",
    "\n",
    "fishnet = gpd.GeoDataFrame(gridinside, columns=['geometry']).set_crs('EPSG:3857')\n",
    "# fishnet.to_file('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_fishnet.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ae5a4",
   "metadata": {},
   "source": [
    "## Sample Tiff files with point centroids - Static Data\n",
    "Change all crs projections to epsg:4326"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef3572",
   "metadata": {},
   "source": [
    "### Point Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open points shapefiles \n",
    "# points shapefiles crs = epsg:3857\n",
    "# tur_shp = gpd.read_file('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_fishnet.shp').to_crs(epsg = 4326)\n",
    "# nar_shp = gpd.read_file('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_fishnet.shp').to_crs(epsg = 4326)\n",
    "\n",
    "# get point centroids to put lat and lon in csv\n",
    "# tur_shp['lon'] = tur_shp['geometry'].x\n",
    "# tur_shp['lat'] = tur_shp['geometry'].y\n",
    "\n",
    "# nar_shp['lon'] = nar_shp['geometry'].x\n",
    "# nar_shp['lat'] = nar_shp['geometry'].y\n",
    "\n",
    "# tur_shp.to_csv('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_fishnet.csv')\n",
    "# nar_shp.to_csv('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_fishnet.csv')\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# pull csv of latitude logitude coordinates of points \n",
    "# turkana\n",
    "# tur_path = '//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//reference_spatial_files//turkana_fishnet.csv'\n",
    "tur_path = 'E:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_fishnet.csv'\n",
    "tur_pts = pd.read_csv(tur_path)\n",
    "\n",
    "tur_lats = list(tur_pts.lat)\n",
    "tur_lons = list(tur_pts.lon)\n",
    "\n",
    "# narok \n",
    "# nar_path = '//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//reference_spatial_files//narok_fishnet.csv'\n",
    "nar_path = 'E:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_fishnet.csv'\n",
    "nar_pts = pd.read_csv(nar_path)\n",
    "\n",
    "nar_lats = list(nar_pts.lat)\n",
    "nar_lons = list(nar_pts.lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ec839",
   "metadata": {},
   "source": [
    "### LULC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c7306bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull LULC raster - crs = epsg:4326\n",
    "lulc = \"F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\Kenya_Sentinel2_LULC2016.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46978d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lulc arguments for turkana  \n",
    "lulc_args_tur = (lulc, tur_lats, tur_lons)\n",
    "lulc_tur = sample_tif(lulc_args_tur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "155edcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe with column from lulc \n",
    "tur_pts['lulc'] = lulc_tur[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c22588db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lulc arguments for turkana  \n",
    "lulc_args_nar = (lulc, nar_lats, nar_lons)\n",
    "lulc_nar = sample_tif(lulc_args_nar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5fcaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe with column from lulc \n",
    "nar_pts['lulc'] = lulc_nar[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0d41f",
   "metadata": {},
   "source": [
    "### SoilGrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e918501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull SoilGrid rasters - crs epsg:7030\n",
    "sg_path = \"F:\\\\Tara_Fall_2019\\\\Kenya_Tanzania_LCC\\\\LCC_Project\\\\Soilgrids_Data\\\\soil_textural_tifs\"\n",
    "sg_files = os.listdir(sg_path)\n",
    "# take full profile rasters only - calculated as weighted average of soil horizons\n",
    "sg_paths = [os.path.join(sg_path, file) for file in sg_files if \"0_200cm\" in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29243239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in main.\n",
      "in pool.\n"
     ]
    }
   ],
   "source": [
    "# soilgrids arguments for turkana\n",
    "sg_args_tur = [(path, tur_lats, tur_lons) for path in sg_paths]\n",
    "\n",
    "arguments = sg_args_tur\n",
    "# PARALLEL \n",
    "if __name__ == '__main__':\n",
    "    print (\"in main.\")\n",
    "    with Pool(14) as pool:\n",
    "        print (\"in pool.\")\n",
    "        sg_result_t = pool.map(sample_tif, arguments)\n",
    "        pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce879ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from sg results - turkana\n",
    "tur_sg_cols = [(pair[0][83:-4]) for pair in sg_result_t]\n",
    "tur_sg_list = [pair[1] for pair in sg_result_t]\n",
    "tur_sg_df = pd.DataFrame(tur_sg_list).transpose()\n",
    "tur_sg_df.columns = tur_sg_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9496d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in main.\n",
      "in pool.\n"
     ]
    }
   ],
   "source": [
    "# soilgrids arguments for narok\n",
    "sg_args_nar = [(path, nar_lats, nar_lons) for path in sg_paths]\n",
    "\n",
    "arguments = sg_args_nar\n",
    "# PARALLEL \n",
    "if __name__ == '__main__':\n",
    "    print (\"in main.\")\n",
    "    with Pool(14) as pool:\n",
    "        print (\"in pool.\")\n",
    "        sg_result_n = pool.map(sample_tif, arguments)\n",
    "        pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af421474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from sg results - turkana\n",
    "nar_sg_cols = [(pair[0][83:-4]) for pair in sg_result_n]\n",
    "nar_sg_list = [pair[1] for pair in sg_result_n]\n",
    "nar_sg_df = pd.DataFrame(nar_sg_list).transpose()\n",
    "nar_sg_df.columns = nar_sg_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c18cfa",
   "metadata": {},
   "source": [
    "### Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbe8919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull slope raster - crs epsg:32636\n",
    "slope = \"F:\\\\DEM\\\\Kenya_slope.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "259946f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope arguments for turkana  \n",
    "slope_args_tur = (slope, tur_lats, tur_lons)\n",
    "slope_tur = sample_tif(slope_args_tur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69ef2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe with column from slope \n",
    "tur_pts['slope'] = slope_tur[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b83fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope arguments for turkana  \n",
    "slope_args_nar = (slope, nar_lats, nar_lons)\n",
    "slope_nar = sample_tif(slope_args_nar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6117e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe with column from slope \n",
    "nar_pts['slope'] = slope_nar[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e425e0",
   "metadata": {},
   "source": [
    "### Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc21a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull elevation raster - crs epsg:32636\n",
    "elev = \"F:\\\\DEM\\\\Kenya_DEM.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffa532de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lulc arguments for turkana  \n",
    "elev_args_tur = (elev, tur_lats, tur_lons)\n",
    "elev_tur = sample_tif(elev_args_tur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b6fd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe with column from slope \n",
    "tur_pts['elevation'] = elev_tur[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8cf7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lulc arguments for turkana  \n",
    "elev_args_nar = (elev, nar_lats, nar_lons)\n",
    "elev_nar = sample_tif(elev_args_nar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83887e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe with column from slope \n",
    "nar_pts['elevation'] = elev_nar[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4911eb3",
   "metadata": {},
   "source": [
    "### Concatenate Static Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7bf07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turkana\n",
    "final_df = pd.concat([tur_pts, tur_sg_df], axis = 1)\n",
    "out_df = final_df[['FID', 'lat', 'lon', 'lulc', 'slope',\n",
    "       'elevation', 'bdod0_200cm_mean', 'cfvo0_200cm_mean', 'clay0_200cm_mean',\n",
    "       'sand0_200cm_mean', 'silt0_200cm_mean', 'soc0_200cm_mean']]\n",
    "out_df.to_csv('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_static_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1fa1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# narok\n",
    "final_df = pd.concat([nar_pts, nar_sg_df], axis = 1)\n",
    "out_df = final_df[['FID', 'lat', 'lon', 'lulc', 'slope',\n",
    "       'elevation', 'bdod0_200cm_mean', 'cfvo0_200cm_mean', 'clay0_200cm_mean',\n",
    "       'sand0_200cm_mean', 'silt0_200cm_mean', 'soc0_200cm_mean']]\n",
    "out_df.to_csv('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_static_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0292c5",
   "metadata": {},
   "source": [
    "## Sample Tiff files with point centroids - Dynamic Data\n",
    "For each date, sample points and create a new dataframe \n",
    "For each date, pull last 6 months of climate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e1afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull dates to then get individual dataframes - TURKANA\n",
    "\n",
    "# pull SAVI z score files -\n",
    "# tur_vi_path = \"//Volumes//Seagate Expansion Drive//bulk_download_USGS//Bulk_Order_Turkana//Landsat_8-9_OLI_TIRS_C2_L2//SAVI_zscore_mosaic\"\n",
    "tur_vi_path = \"E:\\\\bulk_download_USGS\\\\Bulk_Order_Turkana\\\\Landsat_8-9_OLI_TIRS_C2_L2\\\\SAVI_zscore_mosaic\"\n",
    "tur_vi_files = os.listdir(tur_vi_path)\n",
    "tur_savi_files = [os.path.join(tur_vi_path, file) for file in tur_vi_files]\n",
    "\n",
    "# pull dates \n",
    "tur_dates =  [datetime.datetime.strptime(file[:8], '%Y%m%d') for file in tur_vi_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86f49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull dates to then get individual dataframes - NAROK \n",
    "\n",
    "# pull SAVI z score files -\n",
    "# nar_vi_path = \"//Volumes//Seagate Expansion Drive//bulk_download_USGS//Bulk_Order_Maasai_Mara//Landsat_8-9_OLI_TIRS_C2_L2//SAVI_zscore_new\"\n",
    "nar_vi_path = \"E:\\\\bulk_download_USGS\\\\Bulk_Order_Maasai_Mara\\\\Landsat_8-9_OLI_TIRS_C2_L2\\\\SAVI_zscore_new\"\n",
    "nar_vi_files = os.listdir(nar_vi_path)\n",
    "nar_savi_files = [os.path.join(nar_vi_path, file) for file in nar_vi_files]\n",
    "\n",
    "# pull dates \n",
    "nar_dates =  [datetime.datetime.strptime(file[7:15], '%Y%m%d') for file in nar_vi_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e706fa",
   "metadata": {},
   "source": [
    "### Get dataframe for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250c4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precip data starts at 03/01/2014, so start with dates 64 days after that so they have a full record of climate\n",
    "cutoff = tur_dates[9] + datetime.timedelta(days=60)\n",
    "valid_dates_tur = [d for d in tur_dates if d > cutoff]\n",
    "valid_dates_nar = [d for d in nar_dates if d > cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588c52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ### Get SAVI values sampled for each date - then merged to the full dataframe eventually \n",
    "# # datewise arguments for turkana\n",
    "args_tur = [(date, tur_pts, \"Turkana\") for date in valid_dates_tur]\n",
    "\n",
    "# arguments = args_tur\n",
    "# # PARALLEL \n",
    "# if __name__ == '__main__':\n",
    "#     print (\"in main.\")\n",
    "#     with Pool(10) as pool:\n",
    "#         print (\"in pool.\")\n",
    "#         date_result_t = pool.map(get_SAVI_date_df, arguments)\n",
    "#         pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46130c69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-05-08 00:00:00  done.\n",
      "2014-05-15 00:00:00  done.\n",
      "2014-05-17 00:00:00  done.\n",
      "2014-05-24 00:00:00  done.\n"
     ]
    }
   ],
   "source": [
    "date_result_t = []\n",
    "for arg in args_tur: \n",
    "    date_result_t.append(get_SAVI_date_df(arg))\n",
    "    print (arg[0], \" done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d320ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from date results \n",
    "tur_dynamic_SAVI = pd.concat(date_result_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3566ca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put into dask dataframe to handle size\n",
    "from dask import dataframe as dd\n",
    "tur_dyn_dd = dd.from_pandas(tur_dynamic_SAVI, npartitions = 2022)\n",
    "# send to parquet file storage \n",
    "name_function = lambda x: f\"tur_dyn_SAVI-{x}.parquet\"\n",
    "tur_dyn_dd.to_parquet('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_parquet', name_function=name_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd65d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ### Get SAVI values sampled for each date - then merged to the full dataframe eventually \n",
    "# # datewise arguments for narok\n",
    "# args_nar = [(date, nar_pts, \"Narok\") for date in valid_dates_nar]\n",
    "\n",
    "# arguments = args_nar\n",
    "# # PARALLEL \n",
    "# if __name__ == '__main__':\n",
    "#     print (\"in main.\")\n",
    "#     with Pool(14) as pool:\n",
    "#         print (\"in pool.\")\n",
    "#         date_result_n = pool.map(get_SAVI_date_df, arguments)\n",
    "#         pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a13ee64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-05-17 00:00:00  done.\n",
      "2014-06-02 00:00:00  done.\n",
      "2014-06-18 00:00:00  done.\n",
      "2014-07-04 00:00:00  done.\n",
      "2014-07-20 00:00:00  done.\n",
      "2014-08-05 00:00:00  done.\n",
      "2014-08-21 00:00:00  done.\n",
      "2014-10-08 00:00:00  done.\n",
      "2014-10-24 00:00:00  done.\n",
      "2014-11-09 00:00:00  done.\n",
      "2014-11-25 00:00:00  done.\n",
      "2014-12-11 00:00:00  done.\n",
      "2014-12-27 00:00:00  done.\n",
      "2015-01-28 00:00:00  done.\n",
      "2015-02-13 00:00:00  done.\n",
      "2015-03-01 00:00:00  done.\n",
      "2015-03-17 00:00:00  done.\n",
      "2015-04-02 00:00:00  done.\n",
      "2015-05-04 00:00:00  done.\n",
      "2015-06-21 00:00:00  done.\n",
      "2015-07-07 00:00:00  done.\n",
      "2015-08-24 00:00:00  done.\n",
      "2015-09-09 00:00:00  done.\n",
      "2015-10-11 00:00:00  done.\n",
      "2015-11-28 00:00:00  done.\n",
      "2015-12-30 00:00:00  done.\n",
      "2016-02-16 00:00:00  done.\n",
      "2016-03-03 00:00:00  done.\n",
      "2016-03-19 00:00:00  done.\n",
      "2016-04-20 00:00:00  done.\n",
      "2016-05-22 00:00:00  done.\n",
      "2016-06-07 00:00:00  done.\n",
      "2016-07-09 00:00:00  done.\n",
      "2016-07-25 00:00:00  done.\n",
      "2016-08-26 00:00:00  done.\n",
      "2016-09-11 00:00:00  done.\n",
      "2016-09-27 00:00:00  done.\n",
      "2016-10-13 00:00:00  done.\n",
      "2016-10-29 00:00:00  done.\n",
      "2016-11-14 00:00:00  done.\n",
      "2016-11-30 00:00:00  done.\n",
      "2016-12-16 00:00:00  done.\n",
      "2017-01-17 00:00:00  done.\n",
      "2017-02-02 00:00:00  done.\n",
      "2017-02-18 00:00:00  done.\n",
      "2017-03-06 00:00:00  done.\n",
      "2017-03-22 00:00:00  done.\n",
      "2017-04-23 00:00:00  done.\n",
      "2017-05-25 00:00:00  done.\n",
      "2017-06-10 00:00:00  done.\n",
      "2017-08-29 00:00:00  done.\n",
      "2017-09-30 00:00:00  done.\n",
      "2017-10-16 00:00:00  done.\n",
      "2017-11-01 00:00:00  done.\n",
      "2017-11-17 00:00:00  done.\n",
      "2017-12-03 00:00:00  done.\n",
      "2018-02-05 00:00:00  done.\n",
      "2018-03-09 00:00:00  done.\n",
      "2018-03-25 00:00:00  done.\n",
      "2018-04-10 00:00:00  done.\n",
      "2018-05-12 00:00:00  done.\n",
      "2018-05-28 00:00:00  done.\n",
      "2018-07-31 00:00:00  done.\n",
      "2018-09-01 00:00:00  done.\n",
      "2018-09-17 00:00:00  done.\n",
      "2018-10-03 00:00:00  done.\n",
      "2018-11-04 00:00:00  done.\n",
      "2018-12-22 00:00:00  done.\n",
      "2019-01-07 00:00:00  done.\n",
      "2019-02-08 00:00:00  done.\n",
      "2019-03-12 00:00:00  done.\n",
      "2019-03-28 00:00:00  done.\n",
      "2019-04-13 00:00:00  done.\n",
      "2019-04-29 00:00:00  done.\n",
      "2019-07-18 00:00:00  done.\n",
      "2019-08-03 00:00:00  done.\n",
      "2019-09-20 00:00:00  done.\n",
      "2019-10-22 00:00:00  done.\n",
      "2019-11-07 00:00:00  done.\n",
      "2020-01-10 00:00:00  done.\n",
      "2020-02-11 00:00:00  done.\n",
      "2020-03-14 00:00:00  done.\n",
      "2020-03-30 00:00:00  done.\n",
      "2020-04-15 00:00:00  done.\n",
      "2020-06-02 00:00:00  done.\n",
      "2020-06-18 00:00:00  done.\n",
      "2020-07-04 00:00:00  done.\n",
      "2020-08-05 00:00:00  done.\n",
      "2020-09-22 00:00:00  done.\n",
      "2020-10-08 00:00:00  done.\n",
      "2020-10-24 00:00:00  done.\n",
      "2020-11-25 00:00:00  done.\n",
      "2020-12-11 00:00:00  done.\n",
      "2020-12-27 00:00:00  done.\n",
      "2021-01-28 00:00:00  done.\n",
      "2021-03-01 00:00:00  done.\n",
      "2021-03-17 00:00:00  done.\n",
      "2021-04-02 00:00:00  done.\n",
      "2021-04-18 00:00:00  done.\n",
      "2021-05-20 00:00:00  done.\n",
      "2021-07-23 00:00:00  done.\n",
      "2021-08-24 00:00:00  done.\n",
      "2021-11-12 00:00:00  done.\n",
      "2021-12-14 00:00:00  done.\n"
     ]
    }
   ],
   "source": [
    "args_nar = [(date, nar_pts, \"Narok\") for date in valid_dates_nar]\n",
    "date_result_n = []\n",
    "for arg in args_nar: \n",
    "    date_result_n.append(get_SAVI_date_df(arg))\n",
    "    print (arg[0], \" done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee12f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from date results \n",
    "nar_dynamic_SAVI = pd.concat(date_result_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a1b87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put into dask dataframe to handle size\n",
    "from dask import dataframe as dd\n",
    "nar_dyn_dd = dd.from_pandas(nar_dynamic_SAVI, npartitions = 133)\n",
    "# send to parquet file storage \n",
    "name_function = lambda x: f\"nar_dyn_SAVI-{x}.parquet\"\n",
    "nar_dyn_dd.to_parquet('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_parquet', name_function=name_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2cfb64",
   "metadata": {},
   "source": [
    "## Get zscore + climate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e58835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in main.\n",
      "in pool.\n"
     ]
    }
   ],
   "source": [
    "# datewise arguments for turkana - pulling zscore and climate \n",
    "args_tur = [(date, tur_pts, \"Turkana\") for date in valid_dates_tur]\n",
    "\n",
    "arguments = args_tur\n",
    "# PARALLEL \n",
    "if __name__ == '__main__':\n",
    "    print (\"in main.\")\n",
    "    with Pool(14) as pool:\n",
    "        print (\"in pool.\")\n",
    "        date_result_t = pool.map(get_date_df, arguments)\n",
    "        pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from date results \n",
    "tur_dynamic = pd.concat(date_result_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into dask dataframe to handle size\n",
    "from dask import dataframe as dd\n",
    "tur_dyn_dd = dd.from_pandas(tur_dynamic, npartitions = 2022)\n",
    "# send to parquet file storage \n",
    "name_function = lambda x: f\"tur_dyn_new-{x}.parquet\"\n",
    "tur_dyn_dd.to_parquet('E:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_parquet', name_function=name_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17210efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output \n",
    "# tur_dynamic.to_csv('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_dynamic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datewise arguments for turkana\n",
    "args_nar = [(date, nar_pts, \"Narok\") for date in valid_dates_nar]\n",
    "\n",
    "arguments = args_nar\n",
    "# PARALLEL \n",
    "if __name__ == '__main__':\n",
    "    print (\"in main.\")\n",
    "    with Pool(14) as pool:\n",
    "        print (\"in pool.\")\n",
    "        date_result_n = pool.map(get_date_df, arguments)\n",
    "        pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ebb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from date results \n",
    "nar_dynamic = pd.concat(date_result_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into dask dataframe to handle size\n",
    "from dask import dataframe as dd\n",
    "nar_dyn_dd = dd.from_pandas(nar_dynamic, npartitions = 133)\n",
    "# send to parquet file storage \n",
    "name_function = lambda x: f\"nar_dyn_new-{x}.parquet\"\n",
    "nar_dyn_dd.to_parquet('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_parquet', name_function=name_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7edd3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output \n",
    "# nar_dynamic.to_csv('F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_dynamic_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b2672",
   "metadata": {},
   "source": [
    "### Merge Static With Dynamics using Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37dc8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static data \n",
    "# turkana files\n",
    "tur_stat = dd.read_csv('//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//reference_spatial_files//turkana_static_data.csv').drop(\"Unnamed: 0\", axis = 1)\n",
    "# tur_stat = tur_stat.set_index('FID')\n",
    "# narok files\n",
    "nar_stat = dd.read_csv('//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//reference_spatial_files//narok_static_data.csv').drop(\"Unnamed: 0\", axis = 1)\n",
    "# nar_stat = nar_stat.set_index('FID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f73ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic data\n",
    "# turkana files\n",
    "tur_path = '//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//reference_spatial_files//turkana_parquet'\n",
    "tur_files = [os.path.join(tur_path, file) for file in os.listdir(tur_path) if '_dyn' in file and '_SAVI' not in file]\n",
    "tur_df = dd.read_parquet(tur_files)\n",
    "tur_clean = tur_df.dropna(how = 'any')\n",
    "\n",
    "tur_files_savi = [os.path.join(tur_path, file) for file in os.listdir(tur_path) if '_SAVI' in file]\n",
    "tur_df_savi = dd.read_parquet(tur_files_savi)\n",
    "tur_clean_savi = tur_df_savi.dropna(how = 'any')\n",
    "\n",
    "# narok files \n",
    "nar_path = '//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//reference_spatial_files//narok_parquet'\n",
    "nar_files = [os.path.join(nar_path, file) for file in os.listdir(nar_path) if '_dyn' in file and '_SAVI' not in file]\n",
    "nar_df = dd.read_parquet(nar_files)\n",
    "nar_clean = nar_df.dropna(how = 'any')\n",
    "\n",
    "nar_files_savi = [os.path.join(nar_path, file) for file in os.listdir(nar_path) if '_SAVI' in file]\n",
    "nar_df_savi = dd.read_parquet(nar_files_savi)\n",
    "nar_df_savi = nar_df_savi.replace(-255., np.nan)\n",
    "nar_clean_savi = nar_df_savi.dropna(how = 'any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6e86700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index to be the date, then partition by the date so each partition is one date \n",
    "tur_by_date = tur_clean_savi.set_index('date', divisions=sorted(valid_dates_tur), compute = True)\n",
    "nar_by_date = nar_clean_savi.set_index('date', divisions=sorted(valid_dates_nar), compute = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca6ef6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_z_by_date = tur_clean.set_index('date', divisions=sorted(valid_dates_tur), compute = True)\n",
    "nar_z_by_date = nar_clean.set_index('date', divisions=sorted(valid_dates_nar), compute = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73d2d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge savi to dynamic \n",
    "tur_dyn = dd.merge(tur_by_date, tur_z_by_date, left_on=['lat', 'lon', 'date'], right_on=['lat', 'lon', 'date'])\n",
    "nar_dyn = dd.merge(nar_by_date, nar_z_by_date, left_on=['lat', 'lon', 'date'], right_on=['lat', 'lon', 'date'])\n",
    "# reset index so you dont lose date information \n",
    "tur_dyn_reset = tur_dyn.reset_index()\n",
    "nar_dyn_reset = nar_dyn.reset_index()\n",
    "# merge static to dynamic via FID or indices \n",
    "tur_merged = dd.merge(tur_dyn_reset, tur_stat, left_on=['lat', 'lon'], right_on=['lat', 'lon'])\n",
    "nar_merged = dd.merge(nar_dyn_reset, nar_stat, left_on=['lat', 'lon'], right_on=['lat', 'lon'])\n",
    "# # pull lulc of interest, drop the rest\n",
    "tur_all = tur_merged[tur_merged.lulc.isin([1,2,3,4])]\n",
    "nar_all = nar_merged[nar_merged.lulc.isin([1,2,3,4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64071787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>SAVI_zscore</th>\n",
       "      <th>16_day_sum_ppt</th>\n",
       "      <th>32_day_sum_ppt</th>\n",
       "      <th>48_day_sum_ppt</th>\n",
       "      <th>64_day_sum_ppt</th>\n",
       "      <th>16_day_mean_ppt</th>\n",
       "      <th>...</th>\n",
       "      <th>FID</th>\n",
       "      <th>lulc</th>\n",
       "      <th>slope</th>\n",
       "      <th>elevation</th>\n",
       "      <th>bdod0_200cm_mean</th>\n",
       "      <th>cfvo0_200cm_mean</th>\n",
       "      <th>clay0_200cm_mean</th>\n",
       "      <th>sand0_200cm_mean</th>\n",
       "      <th>silt0_200cm_mean</th>\n",
       "      <th>soc0_200cm_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-08-05</td>\n",
       "      <td>-1.830582</td>\n",
       "      <td>35.798562</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>0.081756</td>\n",
       "      <td>0.380693</td>\n",
       "      <td>0.705438</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>2.276737</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>...</td>\n",
       "      <td>186472</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.147817</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>119.950</td>\n",
       "      <td>71.925</td>\n",
       "      <td>334.600</td>\n",
       "      <td>430.575</td>\n",
       "      <td>234.825</td>\n",
       "      <td>260.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-08-05</td>\n",
       "      <td>-1.828337</td>\n",
       "      <td>35.798562</td>\n",
       "      <td>0.298676</td>\n",
       "      <td>0.085019</td>\n",
       "      <td>0.380693</td>\n",
       "      <td>0.705438</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>2.276737</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>...</td>\n",
       "      <td>186473</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.365385</td>\n",
       "      <td>2053.0</td>\n",
       "      <td>119.950</td>\n",
       "      <td>56.425</td>\n",
       "      <td>342.625</td>\n",
       "      <td>427.100</td>\n",
       "      <td>230.250</td>\n",
       "      <td>264.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-08-05</td>\n",
       "      <td>-1.826092</td>\n",
       "      <td>35.798562</td>\n",
       "      <td>0.354735</td>\n",
       "      <td>0.071692</td>\n",
       "      <td>0.380693</td>\n",
       "      <td>0.705438</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>2.276737</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>...</td>\n",
       "      <td>186474</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.144330</td>\n",
       "      <td>2051.0</td>\n",
       "      <td>120.725</td>\n",
       "      <td>61.550</td>\n",
       "      <td>337.575</td>\n",
       "      <td>436.225</td>\n",
       "      <td>225.700</td>\n",
       "      <td>265.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-08-05</td>\n",
       "      <td>-1.823848</td>\n",
       "      <td>35.798562</td>\n",
       "      <td>0.359277</td>\n",
       "      <td>0.395288</td>\n",
       "      <td>0.380693</td>\n",
       "      <td>0.705438</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>2.276737</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>...</td>\n",
       "      <td>186475</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.449230</td>\n",
       "      <td>2065.0</td>\n",
       "      <td>120.600</td>\n",
       "      <td>74.500</td>\n",
       "      <td>321.725</td>\n",
       "      <td>446.875</td>\n",
       "      <td>231.900</td>\n",
       "      <td>264.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-08-05</td>\n",
       "      <td>-1.821603</td>\n",
       "      <td>35.798562</td>\n",
       "      <td>0.363497</td>\n",
       "      <td>0.625477</td>\n",
       "      <td>0.380693</td>\n",
       "      <td>0.705438</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>2.276737</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>...</td>\n",
       "      <td>186476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.288690</td>\n",
       "      <td>2069.0</td>\n",
       "      <td>121.050</td>\n",
       "      <td>78.825</td>\n",
       "      <td>325.325</td>\n",
       "      <td>451.875</td>\n",
       "      <td>222.975</td>\n",
       "      <td>276.850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date       lat        lon      SAVI  SAVI_zscore  16_day_sum_ppt  \\\n",
       "0 2014-08-05 -1.830582  35.798562  0.243137     0.081756        0.380693   \n",
       "1 2014-08-05 -1.828337  35.798562  0.298676     0.085019        0.380693   \n",
       "2 2014-08-05 -1.826092  35.798562  0.354735     0.071692        0.380693   \n",
       "3 2014-08-05 -1.823848  35.798562  0.359277     0.395288        0.380693   \n",
       "4 2014-08-05 -1.821603  35.798562  0.363497     0.625477        0.380693   \n",
       "\n",
       "   32_day_sum_ppt  48_day_sum_ppt  64_day_sum_ppt  16_day_mean_ppt  ...  \\\n",
       "0        0.705438        0.829949        2.276737         0.023793  ...   \n",
       "1        0.705438        0.829949        2.276737         0.023793  ...   \n",
       "2        0.705438        0.829949        2.276737         0.023793  ...   \n",
       "3        0.705438        0.829949        2.276737         0.023793  ...   \n",
       "4        0.705438        0.829949        2.276737         0.023793  ...   \n",
       "\n",
       "      FID  lulc      slope  elevation  bdod0_200cm_mean  cfvo0_200cm_mean  \\\n",
       "0  186472   2.0  12.147817     2100.0           119.950            71.925   \n",
       "1  186473   2.0  25.365385     2053.0           119.950            56.425   \n",
       "2  186474   1.0  30.144330     2051.0           120.725            61.550   \n",
       "3  186475   1.0  21.449230     2065.0           120.600            74.500   \n",
       "4  186476   1.0   7.288690     2069.0           121.050            78.825   \n",
       "\n",
       "   clay0_200cm_mean  sand0_200cm_mean  silt0_200cm_mean  soc0_200cm_mean  \n",
       "0           334.600           430.575           234.825          260.475  \n",
       "1           342.625           427.100           230.250          264.800  \n",
       "2           337.575           436.225           225.700          265.750  \n",
       "3           321.725           446.875           231.900          264.225  \n",
       "4           325.325           451.875           222.975          276.850  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nar_all.partitions[5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79dde71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to parquet file storage \n",
    "name_function = lambda x: f\"tur_all-{x}.parquet\"\n",
    "tur_all.to_parquet('//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//reference_spatial_files//turkana_parquet', name_function=name_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b01300bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to parquet file storage \n",
    "name_function = lambda x: f\"nar_all-{x}.parquet\"\n",
    "nar_all.to_parquet('//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//reference_spatial_files//narok_parquet', name_function=name_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61686da",
   "metadata": {},
   "source": [
    "### Get z score of climate variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ee607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import parquet files into dask dataframes to make for easier processing \n",
    "# lulc = [1,2,3,4]\n",
    "# lulc_strings = [\"tree\", \"shrub\", \"grass\", \"crop\"]\n",
    "\n",
    "# turkana files\n",
    "# tur_path = '//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//turk_moving_window_regr//turkana_parquet'\n",
    "tur_path = \"E:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_parquet\"\n",
    "tur_files = [os.path.join(tur_path, file) for file in os.listdir(tur_path) if '_all' in file]\n",
    "turkana = dd.read_parquet(tur_files) \n",
    "# narok files \n",
    "# nar_path = '//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//nar_moving_window_regr//narok_parquet'\n",
    "nar_path = \"E:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_parquet\"\n",
    "nar_files = [os.path.join(nar_path, file) for file in os.listdir(nar_path) if '_all' in file]\n",
    "narok = dd.read_parquet(nar_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fa398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create year and month columns \n",
    "turkana[\"year\"] = turkana.date.dt.year\n",
    "turkana[\"month\"] = turkana.date.dt.month\n",
    "\n",
    "narok[\"year\"] = narok.date.dt.year\n",
    "narok[\"month\"] = narok.date.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset by LULC for turkana and narok data -  one new dask dataframe for each land use type \n",
    "turkana_lulc = []\n",
    "narok_lulc = []\n",
    "for l in [1,2,3,4]: \n",
    "    turkana_lulc.append(turkana[turkana.lulc == l])\n",
    "    narok_lulc.append(narok[narok.lulc == l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9438a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each location's monthly mean climate variables\n",
    "clim_month_mean_tur = [df.groupby(['FID', 'month']).mean().reset_index()[['month', 'FID', '16_day_sum_ppt',\n",
    "       '32_day_sum_ppt', '48_day_sum_ppt', '64_day_sum_ppt', '16_day_mean_temp', '32_day_mean_temp',\n",
    "       '48_day_mean_temp', '64_day_mean_temp']] for df in turkana_lulc]\n",
    "clim_month_mean_nar = [df.groupby(['FID', 'month']).mean().reset_index()[['month', 'FID', '16_day_sum_ppt',\n",
    "       '32_day_sum_ppt', '48_day_sum_ppt', '64_day_sum_ppt', '16_day_mean_temp', '32_day_mean_temp',\n",
    "       '48_day_mean_temp', '64_day_mean_temp']] for df in narok_lulc]\n",
    "\n",
    "# get each location's monthly stdv of  climate variables\n",
    "clim_month_stdv_tur = [df.groupby(['FID', 'month']).std().reset_index()[['month', 'FID', '16_day_sum_ppt',\n",
    "       '32_day_sum_ppt', '48_day_sum_ppt', '64_day_sum_ppt', '16_day_mean_temp', '32_day_mean_temp',\n",
    "       '48_day_mean_temp', '64_day_mean_temp']] for df in turkana_lulc]\n",
    "clim_month_stdv_nar = [df.groupby(['FID', 'month']).std().reset_index()[['month', 'FID', '16_day_sum_ppt',\n",
    "       '32_day_sum_ppt', '48_day_sum_ppt', '64_day_sum_ppt', '16_day_mean_temp', '32_day_mean_temp',\n",
    "       '48_day_mean_temp', '64_day_mean_temp']] for df in narok_lulc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44185cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate z score of climate variables for turkana and narok based on month and FID specific means and stdvs \n",
    "# merge together the means and stdvs to regular dfs\n",
    "merged_tur = []\n",
    "merged_nar = []\n",
    "\n",
    "for i in range(4): \n",
    "    merged1 = turkana_lulc[i].merge(clim_month_mean_tur[i], how = \"left\", on = [\"FID\", \"month\"])\n",
    "    merged2 = merged1.merge(clim_month_stdv_tur[i], how = \"left\", on = [\"FID\", \"month\"])\n",
    "    merged_tur.append(merged2)\n",
    "for i in range(4): \n",
    "    merged1 = narok_lulc[i].merge(clim_month_mean_nar[i], how = \"left\", on = [\"FID\", \"month\"])\n",
    "    merged2 = merged1.merge(clim_month_stdv_nar[i], how = \"left\", on = [\"FID\", \"month\"])\n",
    "    merged_nar.append(merged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new climate variables that are z scores for that point, given the monthly means for that point \n",
    "z_clim_tur = []\n",
    "z_clim_nar = []\n",
    "for df in merged_tur: \n",
    "    df[\"16_day_sum_ppt_Z\"] = (df[\"16_day_sum_ppt_x\"] - df[\"16_day_sum_ppt_y\"]) / df[\"16_day_sum_ppt\"]\n",
    "    df[\"32_day_sum_ppt_Z\"] = (df[\"32_day_sum_ppt_x\"] - df[\"32_day_sum_ppt_y\"]) / df[\"32_day_sum_ppt\"]\n",
    "    df[\"48_day_sum_ppt_Z\"] = (df[\"48_day_sum_ppt_x\"] - df[\"48_day_sum_ppt_y\"]) / df[\"48_day_sum_ppt\"]\n",
    "    df[\"64_day_sum_ppt_Z\"] = (df[\"64_day_sum_ppt_x\"] - df[\"64_day_sum_ppt_y\"]) / df[\"64_day_sum_ppt\"]\n",
    "    df[\"16_day_mean_temp_Z\"] = (df[\"16_day_mean_temp_x\"] - df[\"16_day_mean_temp_y\"]) / df[\"16_day_mean_temp\"]\n",
    "    df[\"32_day_mean_temp_Z\"] = (df[\"32_day_mean_temp_x\"] - df[\"32_day_mean_temp_y\"]) / df[\"32_day_mean_temp\"]\n",
    "    df[\"48_day_mean_temp_Z\"] = (df[\"48_day_mean_temp_x\"] - df[\"48_day_mean_temp_y\"]) / df[\"48_day_mean_temp\"]\n",
    "    df[\"64_day_mean_temp_Z\"] = (df[\"64_day_mean_temp_x\"] - df[\"64_day_mean_temp_y\"]) / df[\"64_day_mean_temp\"]\n",
    "    out = df.drop(['16_day_sum_ppt_y', '32_day_sum_ppt_y', '48_day_sum_ppt_y',\n",
    "       '64_day_sum_ppt_y', '16_day_mean_temp_y', '32_day_mean_temp_y',\n",
    "       '48_day_mean_temp_y', '64_day_mean_temp_y', '16_day_sum_ppt',\n",
    "       '32_day_sum_ppt', '48_day_sum_ppt', '64_day_sum_ppt',\n",
    "       '16_day_mean_temp', '32_day_mean_temp', '48_day_mean_temp',\n",
    "       '64_day_mean_temp'], axis = 1)\n",
    "    z_clim_tur.append(out)\n",
    "    \n",
    "for df in merged_nar: \n",
    "    df[\"16_day_sum_ppt_Z\"] = (df[\"16_day_sum_ppt_x\"] - df[\"16_day_sum_ppt_y\"]) / df[\"16_day_sum_ppt\"]\n",
    "    df[\"32_day_sum_ppt_Z\"] = (df[\"32_day_sum_ppt_x\"] - df[\"32_day_sum_ppt_y\"]) / df[\"32_day_sum_ppt\"]\n",
    "    df[\"48_day_sum_ppt_Z\"] = (df[\"48_day_sum_ppt_x\"] - df[\"48_day_sum_ppt_y\"]) / df[\"48_day_sum_ppt\"]\n",
    "    df[\"64_day_sum_ppt_Z\"] = (df[\"64_day_sum_ppt_x\"] - df[\"64_day_sum_ppt_y\"]) / df[\"64_day_sum_ppt\"]\n",
    "    df[\"16_day_mean_temp_Z\"] = (df[\"16_day_mean_temp_x\"] - df[\"16_day_mean_temp_y\"]) / df[\"16_day_mean_temp\"]\n",
    "    df[\"32_day_mean_temp_Z\"] = (df[\"32_day_mean_temp_x\"] - df[\"32_day_mean_temp_y\"]) / df[\"32_day_mean_temp\"]\n",
    "    df[\"48_day_mean_temp_Z\"] = (df[\"48_day_mean_temp_x\"] - df[\"48_day_mean_temp_y\"]) / df[\"48_day_mean_temp\"]\n",
    "    df[\"64_day_mean_temp_Z\"] = (df[\"64_day_mean_temp_x\"] - df[\"64_day_mean_temp_y\"]) / df[\"64_day_mean_temp\"]\n",
    "    out = df.drop(['16_day_sum_ppt_y', '32_day_sum_ppt_y', '48_day_sum_ppt_y',\n",
    "       '64_day_sum_ppt_y', '16_day_mean_temp_y', '32_day_mean_temp_y',\n",
    "       '48_day_mean_temp_y', '64_day_mean_temp_y', '16_day_sum_ppt',\n",
    "       '32_day_sum_ppt', '48_day_sum_ppt', '64_day_sum_ppt',\n",
    "       '16_day_mean_temp', '32_day_mean_temp', '48_day_mean_temp',\n",
    "       '64_day_mean_temp'], axis = 1)\n",
    "    z_clim_nar.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc = [\"tree\", \"shrub\", \"grass\", \"crop\"]\n",
    "i = 0\n",
    "for df in z_clim_tur: \n",
    "    # specify parquet path\n",
    "#     parquet_path = '//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//turk_ppt_strat_regr//turkana_parquet'\n",
    "    parquet_path = \"E:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\turkana_parquet\"\n",
    "    # get name function based on lulc \n",
    "    if lulc[i] == \"tree\":\n",
    "        name_function = lambda x: f\"tur_tree-{x}.parquet\"\n",
    "    elif lulc[i] == \"shrub\":\n",
    "        name_function = lambda x: f\"tur_shrub-{x}.parquet\"\n",
    "    elif lulc[i] == \"grass\":\n",
    "        name_function = lambda x: f\"tur_grass-{x}.parquet\"\n",
    "    else:\n",
    "        name_function = lambda x: f\"tur_crop-{x}.parquet\"\n",
    "    # map to new parquet files \n",
    "    df.to_parquet(parquet_path, name_function=name_function)\n",
    "    print (\"Parquet output comlete.\")\n",
    "    i=+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc = [\"tree\", \"shrub\", \"grass\", \"crop\"]\n",
    "i = 0\n",
    "for df in z_clim_nar: \n",
    "    # specify parquet path\n",
    "#     parquet_path = '//Users//taraippolito//Desktop//Tara_Fall_2019//Kenya_Drought//nar_ppt_strat_regr//narok_parquet'\n",
    "    parquet_path = \"E:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\reference_spatial_files\\\\narok_parquet\"\n",
    "    # get name function based on lulc \n",
    "    if lulc[i] == \"tree\":\n",
    "        name_function = lambda x: f\"nar_tree-{x}.parquet\"\n",
    "    elif lulc[i] == \"shrub\":\n",
    "        name_function = lambda x: f\"nar_shrub-{x}.parquet\"\n",
    "    elif lulc[i] == \"grass\":\n",
    "        name_function = lambda x: f\"nar_grass-{x}.parquet\"\n",
    "    else:\n",
    "        name_function = lambda x: f\"nar_crop-{x}.parquet\"\n",
    "    # map to new parquet files \n",
    "    df.to_parquet(parquet_path, name_function=name_function)\n",
    "    print (\"Parquet output comlete.\")\n",
    "    i=+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
