{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3341367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from multiprocessing import Pool\n",
    "from timeit import default_timer as timer\n",
    "from anomaly_fxns import get_dataframes \n",
    "import tqdm\n",
    "from istarmap import istarmap\n",
    "from anomaly_fxns import make_merge_dfs\n",
    "from anomaly_fxns import merge_and_calc_ppt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f22ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nar = pd.read_csv(\"C:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\random_samples\\\\narok_points_completed.csv\").replace(-9999.000000, np.NaN)\n",
    "# tur = pd.read_csv(\"C:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\random_samples\\\\turkana_points_completed.csv\").replace(-9999.000000, np.NaN)\n",
    "\n",
    "nar = pd.read_csv(\"F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\narpts_modis_chirp_chirt_sg.csv\")\n",
    "tur = pd.read_csv(\"F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\turpts_modis_chirp_chirt_sg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d6764",
   "metadata": {},
   "source": [
    "## MODIS new engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e2aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty output dataframes\n",
    "nar_out = nar[['FID', \"Lat\", \"Lon\"]]\n",
    "tur_out = tur[['FID', \"Lat\", \"Lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb911d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted average of soil columns\n",
    "#pass depth as a string value, either 30, 60 or 100\n",
    "def weighted_avg_tex_new(df, depth, var):\n",
    "    #pull all columns \n",
    "    all_cols = df.columns \n",
    "    #pull tex columns \n",
    "    var_cols = [col for col in all_cols if var in col]\n",
    "    if var == \"soc\":\n",
    "        new_col = (5/100)*df[(var+'_0-5cm_mean')] + (10/100)*df[(var+'_5-15cm_mean')] + (15/100)*df[(var+'_15-30cm_mean')] + (30/100)*df[(var+'_30-60cm_mean')] + (40/100)*df[(var+'_60-100cm_mean')]\n",
    "    else: \n",
    "        new_col = (5/100)*df[(var+'_0-5cm_mean')] + (10/100)*df[(var+'_5-15cm_mean')] + (15/100)*df[(var+'_15-30cm_mean')] + (30/100)*df[(var+'_30-60cm_mean')] + (40/100)*df[(var+'_60-100cm_mean')]\n",
    "    #return a series\n",
    "    return (new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73a6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 sand  done.\n",
      "100 sand  done.\n",
      "100 silt  done.\n",
      "100 silt  done.\n",
      "100 clay  done.\n",
      "100 clay  done.\n",
      "100 soc  done.\n",
      "100 soc  done.\n",
      "100 bdod  done.\n",
      "100 bdod  done.\n",
      "100 cfvo  done.\n",
      "100 cfvo  done.\n"
     ]
    }
   ],
   "source": [
    "# create full soil profile \n",
    "# loop over soil variables and depths\n",
    "for var in ['sand', 'silt', 'clay', 'soc', 'bdod', 'cfvo']:\n",
    "    for depth in ['100']:\n",
    "        for df in [nar, tur]:\n",
    "            df[(var + depth)] = weighted_avg_tex_new(df,depth,var)\n",
    "            print (depth, var, \" done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afd4cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean annual temp \n",
    "years = [\"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\"]\n",
    "for df in [nar, tur]:\n",
    "    for year in years: \n",
    "        T_cols = [col for col in df.columns if \"T\" in col]\n",
    "        year_measures = [col for col in T_cols if year in col]\n",
    "        df[(year + \"_avg_temp\")] = df[year_measures].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "972ed1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPP_all = [col for col in nar.columns if \"NPP\" in col ]\n",
    "NPP_tur = [col for col in NPP_all if \".1\" not in col]\n",
    "NPP_nar = [col for col in NPP_all if \".1\" in col ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb143451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NPP columns from appropriate dataframes \n",
    "nar = nar.drop(NPP_tur, axis = 1)\n",
    "tur = tur.drop(NPP_nar, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a1d21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate above ground biomass \n",
    "# AB = NPP * fANPP * treecovermultiplier  * slopesmultiplier \n",
    "def calculate_AB(row, year): \n",
    "    columns = list(row.index)\n",
    "    #pull NPP for a given year\n",
    "    NPP_col = [col for col in columns if \"NPP\" in col and year in col][0]\n",
    "    # pull mean annual temp for given year \n",
    "    temp_col= [col for col in columns if \"_avg_temp\" in col and year in col][0]\n",
    "    # pull LULC  \n",
    "    lulc = row.LULC\n",
    "    # pull slope \n",
    "    slope = row.slope\n",
    "    \n",
    "    \n",
    "    # NPP \n",
    "    NPP = row[NPP_col]\n",
    "    # mean annual temperature \n",
    "    MAT = row[temp_col]\n",
    "    # fraction aboveground npp\n",
    "    fANPP = (0.171) + (0.0129)*MAT \n",
    "    \n",
    "    tree_cover_mult = 1\n",
    "    # tree cover multiplier - this may need to be toggled  \n",
    "    if lulc == 1: \n",
    "        tree_cover_mult = 0\n",
    "    else: \n",
    "        tree_cover_mult = 1\n",
    "\n",
    "    # slope multiplier \n",
    "    slope_multiplier = 1\n",
    "    if slope <= 10: \n",
    "        slope_multiplier = 1\n",
    "    elif (slope > 10) & (slope <= 30):\n",
    "        slope_multiplier = .7\n",
    "    elif (slope > 30) & (slope <= 60):\n",
    "        slope_multiplier = .4 \n",
    "    elif slope > 60:\n",
    "        slope_multiplier = 0 \n",
    "    \n",
    "    \n",
    "    # above ground biomass \n",
    "    AB = NPP * fANPP * tree_cover_mult * slope_multiplier\n",
    "    \n",
    "    \n",
    "    return (AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2dcaab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000  done.\n",
      "2001  done.\n",
      "2002  done.\n",
      "2003  done.\n",
      "2004  done.\n",
      "2005  done.\n",
      "2006  done.\n",
      "2007  done.\n",
      "2008  done.\n",
      "2009  done.\n",
      "2010  done.\n",
      "2011  done.\n",
      "2012  done.\n",
      "2013  done.\n",
      "2014  done.\n",
      "2015  done.\n",
      "2016  done.\n",
      "2000  done.\n",
      "2001  done.\n",
      "2002  done.\n",
      "2005  done.\n",
      "2006  done.\n",
      "2007  done.\n",
      "2008  done.\n",
      "2009  done.\n",
      "2010  done.\n",
      "2011  done.\n",
      "2012  done.\n",
      "2013  done.\n",
      "2014  done.\n",
      "2015  done.\n",
      "2016  done.\n"
     ]
    }
   ],
   "source": [
    "years_t = [\"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\"]\n",
    "for y in years_t: \n",
    "    col_new = y + '_AB'\n",
    "    tur[col_new] = tur.apply(calculate_AB, year = y, axis = 1)\n",
    "    print (y, \" done.\")\n",
    "    \n",
    "years_n = [\"2000\", \"2001\", \"2002\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\"]\n",
    "for y in years_n: \n",
    "    col_new = y + '_AB'\n",
    "    tur[col_new] = tur.apply(calculate_AB, year = y, axis = 1)\n",
    "    print (y, \" done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5a14b087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'FID', 'CID', 'Lat', 'Lon', '2000.01.02_8dayppt',\n",
       "       '2000.01.10_8dayppt', '2000.01.18_8dayppt', '2000.01.26_8dayppt',\n",
       "       '2000.02.03_8dayppt',\n",
       "       ...\n",
       "       '2008_avg_temp', '2009_avg_temp', '2010_avg_temp', '2011_avg_temp',\n",
       "       '2012_avg_temp', '2013_avg_temp', '2014_avg_temp', '2015_avg_temp',\n",
       "       '2016_avg_temp', '2000_AB'],\n",
       "      dtype='object', length=2641)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "45f23e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur = tur.drop(['Unnamed: 0.1', 'Unnamed: 0', 'FID', 'CID'], axis = 1)\n",
    "nar = nar.drop([ 'Unnamed: 0', 'FID', 'CID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "165f7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur.to_csv(\"F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\turkana_AB_analysis.csv\")\n",
    "nar.to_csv(\"F:\\\\Tara_Fall_2019\\\\Kenya_Drought\\\\narok_AB_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a037f",
   "metadata": {},
   "source": [
    "## Data Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c157f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix NDVI columns \n",
    "def fix_ndvi(df): \n",
    "    #pull all columns \n",
    "    all_cols = df.columns\n",
    "    #ndvi_columns \n",
    "    ndvi_cols = [col for col in all_cols if \"F\" in col and 'ppt' not in col]\n",
    "    #loop over ndvi columns to mask values outside -1 to 1 with nan\n",
    "    for col in ndvi_cols: \n",
    "        df[col].mask(~df[col].between(-1, 1), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd393910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix dtype\n",
    "tur = tur.astype('float64')\n",
    "nar = nar.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e257913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply ndvi correction function to both dataframes\n",
    "fix_ndvi(nar)\n",
    "fix_ndvi(tur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d007f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted average of soil columns\n",
    "#pass depth as a string value, either 30, 60 or 100\n",
    "def weighted_avg_tex(df, depth, var):\n",
    "    #pull all columns \n",
    "    all_cols = df.columns \n",
    "    #pull tex columns \n",
    "    var_cols = [col for col in all_cols if var in col]\n",
    "    if var == \"soc\":\n",
    "        if depth == '30': \n",
    "            new_col = (5/30)*df[(var+'_0_5cm')] + (10/30)*df[(var+'_5_15cm')] + (15/30)*df[(var+'_15_30c')]\n",
    "        elif depth == '60': \n",
    "            new_col = (5/60)*df[(var+'_0_5cm')] + (10/60)*df[(var+'_5_15cm')] + (15/60)*df[(var+'_15_30c')] + (30/60)*df[(var+'_30_60c')]\n",
    "        elif depth == '100': \n",
    "            new_col = (5/100)*df[(var+'_0_5cm')] + (10/100)*df[(var+'_5_15cm')] + (15/100)*df[(var+'_15_30c')] + (30/100)*df[(var+'_30_60c')] + (40/100)*df[(var+'_60_100')]\n",
    "    else: \n",
    "        if depth == '30': \n",
    "            new_col = (5/30)*df[(var+'_0_5cm')] + (10/30)*df[(var+'_5_15c')] + (15/30)*df[(var+'_15_30')]\n",
    "        elif depth == '60': \n",
    "            new_col = (5/60)*df[(var+'_0_5cm')] + (10/60)*df[(var+'_5_15c')] + (15/60)*df[(var+'_15_30')] + (30/60)*df[(var+'_30_60')]\n",
    "        elif depth == '100': \n",
    "            new_col = (5/100)*df[(var+'_0_5cm')] + (10/100)*df[(var+'_5_15c')] + (15/100)*df[(var+'_15_30')] + (30/100)*df[(var+'_30_60')] + (40/100)*df[(var+'_60_10')]\n",
    "    #return a series\n",
    "    return (new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2f616e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 sand  done.\n",
      "30 sand  done.\n",
      "60 sand  done.\n",
      "60 sand  done.\n",
      "100 sand  done.\n",
      "100 sand  done.\n",
      "30 silt  done.\n",
      "30 silt  done.\n",
      "60 silt  done.\n",
      "60 silt  done.\n",
      "100 silt  done.\n",
      "100 silt  done.\n",
      "30 clay  done.\n",
      "30 clay  done.\n",
      "60 clay  done.\n",
      "60 clay  done.\n",
      "100 clay  done.\n",
      "100 clay  done.\n",
      "30 soc  done.\n",
      "30 soc  done.\n",
      "60 soc  done.\n",
      "60 soc  done.\n",
      "100 soc  done.\n",
      "100 soc  done.\n",
      "30 bdod  done.\n",
      "30 bdod  done.\n",
      "60 bdod  done.\n",
      "60 bdod  done.\n",
      "100 bdod  done.\n",
      "100 bdod  done.\n",
      "30 cfvo  done.\n",
      "30 cfvo  done.\n",
      "60 cfvo  done.\n",
      "60 cfvo  done.\n",
      "100 cfvo  done.\n",
      "100 cfvo  done.\n"
     ]
    }
   ],
   "source": [
    "# loop over soil variables and depths\n",
    "for var in ['sand', 'silt', 'clay', 'soc', 'bdod', 'cfvo']:\n",
    "    for depth in ['30', '60', '100']:\n",
    "        for df in [nar, tur]:\n",
    "            df[(var + depth)] = weighted_avg_tex(df,depth,var)\n",
    "            print (depth, var, \" done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ffbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns which are mssing texture data \n",
    "tur = tur.drop(tur[tur['sand30'] == 0].index)\n",
    "nar = nar.drop(nar[nar['sand30'] == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a941a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b402858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur.to_csv(\"turkana_raw_w_soils.csv\")\n",
    "nar.to_csv(\"narok_raw_w_soils.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa87b9",
   "metadata": {},
   "source": [
    "## Get anomalies NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbdeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rangeland data \n",
    "tur = pd.read_csv(\"turkana_raw_w_soils.csv\")\n",
    "nar = pd.read_csv(\"narok_raw_w_soils.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1a06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull ndvi columns for turkana and narok\n",
    "ndvi_cols_t = [col for col in tur.columns if \"F\" in col and 'ppt' not in col]\n",
    "ndvi_cols_n = [col for col in nar.columns if \"F\" in col and 'ppt' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe592540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dates in the format of precipitation columns\n",
    "ndvi_dates_t = [col[1:5] + \"_\"+ col[5:7]+\"_\"+col[7:] for col in ndvi_cols_t]\n",
    "ndvi_dates_n = [col[1:5] + \"_\"+ col[5:7]+\"_\"+col[7:] for col in ndvi_cols_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a6c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppt_col_list = [col for col in tur.columns if \"ppt\" in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa72a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bones for new ppt dataframes\n",
    "precip_df_t = pd.DataFrame(ndvi_cols_t, columns = ['Date'])\n",
    "precip_df_t['ppt_date'] = ndvi_dates_t\n",
    "\n",
    "precip_df_n = pd.DataFrame(ndvi_cols_n, columns = ['Date'])\n",
    "precip_df_n['ppt_date'] = ndvi_dates_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e138001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean NDVI for each row \n",
    "tur['mean_NDVI'] = tur[ndvi_cols_t].mean(axis = 1)\n",
    "nar['mean_NDVI'] = nar[ndvi_cols_n].mean(axis = 1)\n",
    "tur['stdv_NDVI'] = tur[ndvi_cols_t].std(axis = 1)\n",
    "nar['stdv_NDVI'] = nar[ndvi_cols_n].std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4ff7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_by_date = pd.melt(tur, id_vars=['Unnamed: 0'], value_vars=ndvi_cols_t, var_name='Date', value_name='NDVI_value').dropna()\n",
    "nar_by_date = pd.melt(nar, id_vars=['Unnamed: 0'], value_vars=ndvi_cols_n, var_name='Date', value_name='NDVI_value').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336c059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_plus_mean = tur_by_date.merge(tur[[\"mean_NDVI\", \"stdv_NDVI\", \"cropland\", \"trees\", \"grassland\", \"shrub\", \"Unnamed: 0\"]], how = \"left\", left_on = \"Unnamed: 0\", right_on = \"Unnamed: 0\")\n",
    "nar_plus_mean = nar_by_date.merge(nar[[\"mean_NDVI\", \"stdv_NDVI\", \"cropland\", \"trees\", \"grassland\", \"shrub\", \"Unnamed: 0\"]], how = \"left\", left_on = \"Unnamed: 0\", right_on = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3c6a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = [col for col in tur.columns if \"ppt\" in col or col in [\"Unnamed: 0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffa9eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_plus_mean.name = \"tur_plus_mean\"\n",
    "nar_plus_mean.name = \"nar_plus_mean\"\n",
    "\n",
    "#use the full set\n",
    "base_sets = [tur_plus_mean, nar_plus_mean]\n",
    "\n",
    "#pull unique dates for each \n",
    "dates = [list(tur_plus_mean.Date.unique()), list(nar_plus_mean.Date.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "148e3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_t = [(base_sets[0], date) for date in dates[0]]\n",
    "arguments_n = [(base_sets[1], date) for date in dates[1]]\n",
    "arguments = arguments_t + arguments_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f34da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 526/526 [24:31<00:00,  2.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# PARALLEL SPLIT DATA BY REGION AND DATE\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with Pool(14) as pool:\n",
    "        result = []\n",
    "        for _ in tqdm.tqdm(pool.istarmap(get_dataframes, arguments),\n",
    "                           total=len(arguments)):\n",
    "            result.append(_)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2481dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_date_dfs = result[:420]\n",
    "nar_date_dfs = result[420:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b44cfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for pulling merge specific columns for each date\n",
    "ppt_arg_t = [(tur, date) for date in dates[0]]\n",
    "ppt_arg_n = [(nar, date) for date in dates[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "985050b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 420/420 [32:07<00:00,  4.59s/it]\n"
     ]
    }
   ],
   "source": [
    "arguments = ppt_arg_t\n",
    "# PARALLEL PULL MERGE DFS FOR EACH REGION X DATE\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with Pool(14) as pool:\n",
    "        result_t = []\n",
    "        for _ in tqdm.tqdm(pool.istarmap(make_merge_dfs, arguments),\n",
    "                           total=len(arguments)):\n",
    "            result_t.append(_)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eff04036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 106/106 [07:39<00:00,  4.33s/it]\n"
     ]
    }
   ],
   "source": [
    "arguments = ppt_arg_n\n",
    "# PARALLEL PULL MERGE DFS FOR EACH REGION X DATE\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with Pool(14) as pool:\n",
    "        result_n = []\n",
    "        for _ in tqdm.tqdm(pool.istarmap(make_merge_dfs, arguments),\n",
    "                           total=len(arguments)):\n",
    "            result_n.append(_)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46f277a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_arg_t = list(zip(tur_date_dfs, result_t))\n",
    "final_arg_n = list(zip(nar_date_dfs, result_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce1c125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 420/420 [00:29<00:00, 14.06it/s]\n"
     ]
    }
   ],
   "source": [
    "arguments = final_arg_t\n",
    "# PARALLEL PULL MERGE DFS FOR EACH REGION X DATE\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with Pool(14) as pool:\n",
    "        final_t = []\n",
    "        for _ in tqdm.tqdm(pool.istarmap(merge_and_calc_ppt, arguments),\n",
    "                           total=len(arguments)):\n",
    "            final_t.append(_)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a950414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 106/106 [00:12<00:00,  8.38it/s]\n"
     ]
    }
   ],
   "source": [
    "arguments = final_arg_n\n",
    "# PARALLEL PULL MERGE DFS FOR EACH REGION X DATE\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with Pool(14) as pool:\n",
    "        final_n = []\n",
    "        for _ in tqdm.tqdm(pool.istarmap(merge_and_calc_ppt, arguments),\n",
    "                           total=len(arguments)):\n",
    "            final_n.append(_)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0986cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_T = pd.concat(final_t)\n",
    "FINAL_N = pd.concat(final_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8deb7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_T.to_csv(\"TURKANA_ANOMALIES.csv\")\n",
    "FINAL_N.to_csv(\"NAROK_ANOMALIES.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
